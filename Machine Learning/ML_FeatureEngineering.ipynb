{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "%logstop\n",
    "%logstart -rtq ~/.logs/ML_FeatureEngineering.py append\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "<!-- requirement: data/land_temps_by_city.csv -->\n",
    "\n",
    "Our models can't reveal structure in our data when no structure exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.random.randn(100)\n",
    "x2 = np.random.randn(100)\n",
    "y = np.random.choice([True, False], size=100)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(x1[y], x2[y], 'b.')\n",
    "plt.plot(x1[~y], x2[~y], 'r.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df = pd.DataFrame(np.vstack([x1, x2, y]).T, columns=['x1', 'x2', 'y'])\n",
    "random_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond the choice of features to measure and how much data to collect (which we often can't control), there are many techniques for transforming our data set to amplify the most useful information for our machine learning models. Some of these techniques operate on the data directly while others are modifications to how we train our estimator. Such techniques are collectively called feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "Machine learning algorithm require observations to be vectors of features. Sometimes our raw data will not be a vector of features or a vector at all. For example, our data might be a collection of images. We could flatten the image into a vector, but this destroys important spatial relationships in the data. Our data could be a string of text like a news article. A string is linear, but doesn't have fixed length, and it's not immediately clear how to represent text as numbers. Our data could be several years of commodity price data, which is a vector of numbers, but it may be better represented in an aggregated form instead of a raw time-series.\n",
    "\n",
    "In cases like these we have the privilege of constructing features, giving us maximum control over preparation of the data (without actually deciding how raw data gets collected). Feature extraction will often be specific to the data set, but some things to think about include:\n",
    "\n",
    "- What counts as an observation in the raw data?\n",
    "- Are there spatial or temporal relationships within the data of a single observation?\n",
    "- If my data aren't numbers, what is the most meaningful way to transform the data into numbers?\n",
    "- If my data are numbers, are the raw values most meaningful? Differences between raw values? Other transformations or aggregations?\n",
    "\n",
    "Let's work through an example to illustrate some of these concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = pd.read_csv('./data/land_temps_by_city.csv', parse_dates=[0])\n",
    "temps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have temperature data for many cities around the world. If we were making predictions about the climate of these different cities, it may be more useful to summarize the temperature data in a quantity that characterizes the different climates. For example, some of these cities have 4 seasons while others do not. We might see this reflected in how the temperatures correlate with each other over the course of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "def collect(x):\n",
    "    return reduce(lambda y, z: y + [z] if isinstance(y, list) else [z], x)\n",
    "\n",
    "def estimated_autocorrelation(x):\n",
    "    n = len(x)\n",
    "    variance = x.var()\n",
    "    x = x-x.mean()\n",
    "    r = np.correlate(x, x, mode = 'full')[-n:]\n",
    "    result = r/(variance*(np.arange(n, 0, -1)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(estimated_autocorrelation(temps[temps['City']=='Berlin']['AverageTemperature'].values)[:12], label='Berlin')\n",
    "plt.plot(estimated_autocorrelation(temps[temps['City']=='Jakarta']['AverageTemperature'].values)[:12], label='Jakarta')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these graphs, we can see that the temperature in Berlin six months in the future mirrors the temperature in Berlin today (correlation $\\approx$ -1). However, in Jakarta, the average temperature six months in the future is basically unrelated to the temperature today (correlation $\\approx$ 0). This indicates a different seasonality, associated with their different climates. We can extract the six-month autocorrelation value as a feature for each city's climate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = temps.groupby('City')['AverageTemperature'].apply(collect).apply(lambda x: np.array(x)).apply(estimated_autocorrelation).rename('autocorr')\n",
    "ac_lat = pd.concat([ac.apply(lambda x: x[range(6, 1362, 12)].mean()), temps[['City', 'Latitude']].drop_duplicates().set_index('City')], axis=1)\n",
    "ac_lat['Latitude'] = ac_lat['Latitude'].apply(lambda x: float(x[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ac_lat['Latitude'], ac_lat['autocorr'], '.')\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('6-month temperature autocorrelation');\n",
    "\n",
    "ax = plt.gca()\n",
    "labels = {'Lima', 'Shenzhen', 'Delhi', 'Lima', 'Kingston', 'Cape Town', 'Jakarta', 'Nairobi', 'Rio De Janeiro', 'Quito', 'Port Moresby', 'Seoul', 'Moscow', 'Paris', 'Toronto'}\n",
    "for city in ac_lat.index:\n",
    "    if city in labels:\n",
    "        plt.text(ac_lat.loc[city, 'Latitude'], ac_lat.loc[city, 'autocorr'] + .01, city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature transformations\n",
    "\n",
    "Sometimes our observations will be very unevenly distributed for a given feature. For example, income is roughly exponentially distributed in many populations. Sometimes the relationship between a feature and a regression target follows a clear non-linear trend. In cases like these it can be useful to transform the values of our features or our target to better highlight trends or to allow for use of models that might not otherwise be applicable.\n",
    "\n",
    "For example, we could fit a line to the relationship between latitude and 6-month autocorrelation in temperature, but it wouldn't be a great fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression().fit(ac_lat['Latitude'].values.reshape(-1, 1), ac_lat['autocorr'].values.reshape(-1, 1))\n",
    "\n",
    "ac_lat.sort_values('Latitude', inplace=True)\n",
    "\n",
    "plt.plot(ac_lat['Latitude'], ac_lat['autocorr'], '.')\n",
    "plt.plot(ac_lat['Latitude'], lin_reg.predict(ac_lat['Latitude'].values.reshape(-1, 1)))\n",
    "plt.xlabel(\"Latitude\")\n",
    "plt.ylabel('6-month temperature autocorrelation');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this data looks roughly like $y = e^{-x/\\lambda} - 1$. If we substituted the variable $u = e^{-x/\\lambda}$, we would have $y \\propto u$, a linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.exp(-ac_lat['Latitude']/13), ac_lat['autocorr'], '.')\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('6-month temperature autocorrelation');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks somewhat more linear. We could fit a linear model to this transformed feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(*shuffle(ac_lat['Latitude'].values.reshape(-1, 1), ac_lat['autocorr'].values.reshape(-1, 1)), test_size=0.1)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "lin_reg.fit(np.exp(-X_train/13), y_train)\n",
    "\n",
    "lin_pred = lin_reg.predict(np.exp(-X_test/13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the result with a decision tree applied to the untransformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=3)\n",
    "tree_reg.fit(X_train, y_train)\n",
    "tree_pred = tree_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_lat.sort_values('Latitude', inplace=True)\n",
    "\n",
    "plt.plot(ac_lat['Latitude'], ac_lat['autocorr'], '.', label='data')\n",
    "plt.plot(ac_lat['Latitude'], lin_reg.predict(np.exp(-ac_lat['Latitude'].values.reshape(-1, 1) / 13)))\n",
    "plt.plot(ac_lat['Latitude'], tree_reg.predict(ac_lat['Latitude'].values.reshape(-1, 1)))\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('6-month temperature autocorrelation');\n",
    "\n",
    "print('Transformed linear regression: %0.2f' % mse(y_test, lin_pred))\n",
    "print('Decision tree regression: %0.2f' % mse(y_test, tree_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curse of dimensionality\n",
    "\n",
    "It is easy to see why adding features to a data set might be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "x1 = np.concatenate([np.random.randn(n // 2), np.random.randn(n // 2) + 2])\n",
    "y = np.array([True if i < n // 2 else False for i in range(n)])\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.hist(x1[y], alpha=.8)\n",
    "plt.hist(x1[~y], alpha=.8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = np.concatenate([np.random.randn(n // 2), np.random.randn(n // 2) + 2])\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(x1[y], x2[y], 'b.')\n",
    "plt.plot(x1[~y], x2[~y], 'r.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vstack([x1, x2]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.vstack([x1, x2]).T, y)\n",
    "\n",
    "log_reg_1d = LogisticRegression(solver='lbfgs').fit(np.atleast_2d(X_train[:, 0]).T, y_train)\n",
    "\n",
    "log_reg_2d = LogisticRegression(solver='lbfgs').fit(X_train, y_train)\n",
    "\n",
    "print('Score on 1D data: %0.2f' % accuracy_score(y_test, log_reg_1d.predict(np.atleast_2d(X_test[:, 0]).T)))\n",
    "print('Score on 2D data: %0.2f' % accuracy_score(y_test, log_reg_2d.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "xx1, xx2 = np.meshgrid(np.arange(-4, 6, .1), np.arange(-4, 6, .1))\n",
    "\n",
    "Z = log_reg_2d.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "Z = Z.reshape(xx1.shape)\n",
    "\n",
    "cut_1d = np.arange(-4, 6, .1)[log_reg_1d.predict_proba(np.atleast_2d(np.arange(-4,6,.1)).T)[:, 0] > .5][0]\n",
    "\n",
    "colors = ('red', 'blue')\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "plt.plot(x1[y], x2[y], 'b.')\n",
    "plt.plot(x1[~y], x2[~y], 'r.')\n",
    "plt.axvline(cut_1d, color='k', linestyle='--');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as we increase the dimensionality of our data by adding more features, the data gets more spread out. If the data is very high dimensional, it may be too spread out to identify any trends or clusters. This is know as the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality). As we increase the number of features, we will generally require more observations to ensure good sampling density throughout feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance and selection\n",
    "\n",
    "To avoid the curse of dimensionality, we'll sometimes want to reduce the dimensionality of our data. We may also want to reduce the dimensionality of our data simply to compress it (in the case of very large data) or simply to visualize the data. Reducing dimensionality also helps make models more interpretable. When reducing the dimensionality of the data, we will want to get rid of irrelevant or redundant features.\n",
    "\n",
    "There are a number of ways to assess feature importance. Let's work again with the California housing data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://dataincubator-wqu.s3.amazonaws.com/caldata/cal_housing.pkz -nc -P ~/scikit_learn_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "cali_data = fetch_california_housing()\n",
    "\n",
    "cali_df = pd.DataFrame(cali_data.data, columns=cali_data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "import scipy.stats\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(*shuffle(cali_df, cali_data.target), test_size=0.1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "\n",
    "rs = RandomizedSearchCV(model,\n",
    "                  {'max_depth': scipy.stats.binom(9, .3, loc=1),\n",
    "                  'min_samples_split': scipy.stats.binom(90, .5, loc=10)},\n",
    "                   cv=5,\n",
    "                   n_iter=200,\n",
    "                   n_jobs=4,\n",
    "                   scoring='neg_mean_squared_error')\n",
    "\n",
    "rs.fit(X_train, y_train)\n",
    "print(r2_score(y_test, rs.best_estimator_.predict(scaler.transform(X_test))))\n",
    "\n",
    "list(zip(cali_data.feature_names, rs.best_estimator_.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(*shuffle(cali_df, cali_data.target), test_size=0.1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "lin_reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print(r2_score(y_test, lin_reg.predict(scaler.transform(X_test))))\n",
    "\n",
    "list(zip(cali_data.feature_names, abs(lin_reg.coef_) / sum(abs(lin_reg.coef_))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `feature_selection` submodule of Scikit-learn provides [some useful tools](http://scikit-learn.org/stable/modules/feature_selection.html) for discarding unimportant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "feature_elimination = RFECV(estimator=model, cv=5, scoring='r2')\n",
    "feature_elimination.fit(X_train, y_train)\n",
    "\n",
    "rfecv_scores = feature_elimination.grid_scores_\n",
    "plt.plot(range(1, len(rfecv_scores) + 1), rfecv_scores)\n",
    "plt.xlabel('Number of features used')\n",
    "plt.ylabel(r'Cross validation score ($R^2$)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "When we have too many features, our observations become sparse, making it hard to detect trends in the data. Our model begins to simply memorize the training set based on the many features. We can also say our model is fitting _noise_ instead of trend, because small random variations across many features results in observations being very separated.\n",
    "\n",
    "One way to limit overfitting due to high dimensionality is _regularization_. To regularize a model, we introduce a penalty in the cost function associated with the values of model parameters themselves. For example, we could regularize our linear regression model by changing the cost function.\n",
    "\n",
    "$$ C({\\beta_i}) = \\sum_j (y_j - X_{ij}\\beta_i)^2 \\Longrightarrow\n",
    "C({\\beta_i}) = \\sum_j (y_j - X_{ij}\\beta_i)^2 + \\alpha\\sum_i\\beta_i^2 $$\n",
    "\n",
    "The new cost function will tend to reduce the values of model parameters, limiting the effect of unimportant features. We could choose other penalties (e.g. $\\alpha\\sum_i\\|\\beta_i\\|$) to change the effects of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "X_train, X_test, y_train, y_test = train_test_split(*shuffle(cali_df, cali_data.target), test_size=0.1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "lin_reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "alphas = np.logspace(-2, 2, 100)\n",
    "ridge_coefs = []\n",
    "ridge_scores = []\n",
    "lasso_coefs = []\n",
    "lasso_scores = []\n",
    "for alpha in alphas:\n",
    "    ridge_reg = Ridge(alpha=alpha).fit(X_train, y_train)\n",
    "    lasso_reg = Lasso(alpha=alpha).fit(X_train, y_train)\n",
    "    ridge_coefs.append(ridge_reg.coef_)\n",
    "    ridge_scores.append(r2_score(y_test, ridge_reg.predict(scaler.transform(X_test))))\n",
    "    lasso_coefs.append(lasso_reg.coef_)\n",
    "    lasso_scores.append(r2_score(y_test, lasso_reg.predict(scaler.transform(X_test))))\n",
    "\n",
    "lin_score = r2_score(y_test, lin_reg.predict(scaler.transform(X_test)))\n",
    "print('Linear regression score: %0.2f' % lin_score)\n",
    "print('Ridge regression score: %0.2f' % max(ridge_scores))\n",
    "print('Lasso regression score: %0.2f' % max(lasso_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.gca().set_xscale('log')\n",
    "plt.gca().set_ylim([.5, .625])\n",
    "plt.plot(alphas, np.repeat(lin_score, len(alphas)), label='simple')\n",
    "plt.plot(alphas, ridge_scores, label='ridge')\n",
    "plt.plot(alphas, lasso_scores, label='lasso')\n",
    "plt.xlabel(r'Regularization strength ($\\alpha$)')\n",
    "plt.ylabel(r'$R^2$ score')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12, 8])\n",
    "plt.title('Regression coefficients')\n",
    "\n",
    "plt.subplot(311)\n",
    "plt.gca().set_xscale('log')\n",
    "plt.ylabel('Simple regression coefficients')\n",
    "for coef in lin_reg.coef_:\n",
    "    plt.plot(alphas, np.repeat(coef, len(alphas)))\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.gca().set_xscale('log')\n",
    "plt.ylabel('Ridge regression coefficients')\n",
    "plt.plot(alphas, ridge_coefs)\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.gca().set_xscale('log')\n",
    "plt.ylabel('Lasso regression coefficients')\n",
    "plt.xlabel(r'Regularization strength ($\\alpha$)')\n",
    "plt.plot(alphas, lasso_coefs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 15\n",
    "print(alphas[x], lasso_scores[x])\n",
    "list(zip(cali_data.feature_names, lasso_coefs[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between features and multicollinearity\n",
    "\n",
    "Sometimes our features may be closely related to other features. We can easily calculate the correlation coefficient between pairs of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = cali_df.corr()\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, square=True, linewidths=.5, center=0, vmax=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation can be an indicator of feature redundancy. In particular if a feature can be approximated as a linear combination of other features\n",
    "\n",
    "$$ X_j \\approx \\sum_{i \\ne j}\\beta_{ij}X_i $$\n",
    "\n",
    "we say our features are multicollinear. Multicollinearity can introduce instability into our regression model parameters, making model interpretation difficult. Furthermore, features that are linear combinations of other features are redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis\n",
    "\n",
    "So far we have explored techniques for identifying unimportant or redundant features and ways to eliminate them from our data or model. When we eliminate features from our data, even if they are correlated with other features, we lose information.\n",
    "\n",
    "One way to mitigate the loss of information is to combine our $n$ original features into $m$ new features, with $m < n$. We can calculate what combinations preserve the most information using **principal component analysis** (PCA).\n",
    "\n",
    "Let's look back to an earlier example to understand how this works. We found earlier that adding a second feature to a data set helped us gain predictive power to separate two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "def plot_data(rotation=0):\n",
    "    x1_r = x1 * np.cos(rotation) + x2 * np.sin(rotation)\n",
    "    x2_r = -x1 * np.sin(rotation) + x2 * np.cos(rotation)\n",
    "    plt.plot(x1_r[y], x2_r[y], 'b.')\n",
    "    plt.plot(x1_r[~y], x2_r[~y], 'r.')\n",
    "\n",
    "    w1, w2 = log_reg_2d.coef_[0]\n",
    "    x = np.sort(x1)\n",
    "    line = (-log_reg_2d.intercept_ - w1 * x) / w2\n",
    "    x_r = x * np.cos(rotation) + line * np.sin(rotation)\n",
    "    line_r = -x * np.sin(rotation) + line * np.cos(rotation)\n",
    "    plt.plot(x_r, line_r)\n",
    "    \n",
    "interact(plot_data, rotation=(0, np.pi, .1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After rotating the data, we see that we could reduce the data from two dimensions to one without losing any important information. Principal component analysis projects the data into a lower dimensional space, choosing the projection by maximizing the variance in the projected data. Usually this is the optimal choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.vstack([x1, x2]).T, y)\n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "X_pca_train = pca.fit_transform(X_train)\n",
    "X_pca_test = pca.transform(X_test)\n",
    "\n",
    "log_reg_pca = LogisticRegression(solver='lbfgs').fit(X_pca_train, y_train)\n",
    "\n",
    "print('Score on 2D data: %0.2f' % accuracy_score(y_test, log_reg_2d.predict(X_test)))\n",
    "print('Score on 1D data (PCA): %0.2f' % accuracy_score(y_test, log_reg_pca.predict(X_pca_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble models\n",
    "\n",
    "We began the notebook by exploring feature extraction, which may involve applying complex algorithms to raw data to transform it into useful features for machine learning. An extension of this idea is to use machine learning models to construct features. This brings us to the idea of an _ensemble_ model, a model that uses the predictions of multiple estimators to make a final prediction.\n",
    "\n",
    "One example of an ensemble model is the _random forest_. A random forest model trains many decision trees on random pieces of the data and averages together their predictions. In a sense, the prediction of any individual tree becomes a feature in the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "X_train, X_test, y_train, y_test = train_test_split(*shuffle(cali_df, cali_data.target), test_size=0.1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=50)\n",
    "\n",
    "gs = GridSearchCV(model,\n",
    "                  {'max_features': np.arange(.05, 1, .05)},\n",
    "                  cv=5,\n",
    "                  n_jobs=4,\n",
    "                  scoring='neg_mean_squared_error')\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "model = gs.best_estimator_\n",
    "\n",
    "print(gs.best_params_)\n",
    "print(r2_score(y_test, model.predict(scaler.transform(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(model,\n",
    "                  {'min_samples_leaf': np.arange(1, 50, 5)},\n",
    "                  cv=5,\n",
    "                  n_jobs=4,\n",
    "                  scoring='neg_mean_squared_error')\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "model = gs.best_estimator_\n",
    "\n",
    "print(gs.best_params_)\n",
    "print(r2_score(y_test, model.predict(scaler.transform(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(model,\n",
    "                  {'n_estimators': np.arange(100, 301, 100)},\n",
    "                  cv=5,\n",
    "                  n_jobs=4,\n",
    "                  scoring='neg_mean_squared_error')\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "model = gs.best_estimator_\n",
    "\n",
    "print(gs.best_params_)\n",
    "print(r2_score(y_test, model.predict(scaler.transform(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom ensembles\n",
    "\n",
    "We can also create our own ensembles.\n",
    "\n",
    "This will involve some new tools, _pipelines_ and _feature unions_. Pipelines allow us to bundle a series of transformers and estimators. Feature unions all us to bundle parallel transformers.\n",
    "\n",
    "Remember that our ensemble is not meant to make predictions per se, but rather perform feature extraction, similar to a transformer. Therefore we need the estimators in our ensemble to implement a `transform` method instead of a `predict` method. We can make this modification using a simple custom class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class EstTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.estimator.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.atleast_2d(self.estimator.predict(X)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build a feature union that will use a decision tree regressor and a K-neighbors regressor in parallel to construct our ensemble features. Our pipeline will scale our raw features, pass them to the ensemble, and then make a final prediction using a simple linear regression to combine the ensemble predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(*shuffle(cali_df, cali_data.target), test_size=0.1)\n",
    "\n",
    "knn = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsRegressor())\n",
    "])\n",
    "\n",
    "gs = GridSearchCV(knn,\n",
    "                  {'knn__n_neighbors': range(5, 26, 5)},\n",
    "                  cv=5,\n",
    "                  n_jobs=4,\n",
    "                  scoring='neg_mean_squared_error')\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_params_)\n",
    "print(r2_score(y_test, gs.best_estimator_.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "ensemble = FeatureUnion([\n",
    "    ('rf', EstTransformer(DecisionTreeRegressor(max_depth=5, min_samples_split=45))),\n",
    "    ('knn', EstTransformer(KNeighborsRegressor(n_neighbors=10)))\n",
    "])\n",
    "\n",
    "model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ensemble', ensemble),\n",
    "    ('combine', LinearRegression(fit_intercept=False))\n",
    "    ])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(r2_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2020 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
